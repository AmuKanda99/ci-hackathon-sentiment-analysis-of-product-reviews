{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Sentiment Analysis of Product Reviews - ETL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "### **Objectives**\n",
        "\n",
        "In this notebook we will load the raw dataset 'Reviews.csv' from Kaggle  (https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews). We will perform the ETL(Extract, Transform and Load) process on the dataset by cleaning it, ensuring that all required variables are present and correctly formatted, and then saving the resulting 'cleaned' dataset for subsequent analyses.\n",
        "\n",
        "### **Inputs**\n",
        "\n",
        "Raw dataset from Kaggle 'Reviews.csv'\n",
        "\n",
        "### **Outputs**\n",
        "\n",
        "Cleaned dataset saved as a new CSV file.\n",
        "\n",
        "### **Prerequisites**\n",
        "\n",
        "* Python 3.12.8 is installed\n",
        "* Required Python libraries from requirement.txt and their dependencies must be installed\n",
        "* Optional to set up Python virtual environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extract: Import data and libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "### Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\admin\\\\Documents\\\\vscode-projects\\\\ci-hackathon-sentiment-analysis-of-product-reviews\\\\jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\admin\\\\Documents\\\\vscode-projects\\\\ci-hackathon-sentiment-analysis-of-product-reviews'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(568454, 10)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
              "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      5  1303862400   \n",
              "1                     0                       0      1  1346976000   \n",
              "2                     1                       1      4  1219017600   \n",
              "3                     3                       3      2  1307923200   \n",
              "4                     0                       0      5  1350777600   \n",
              "\n",
              "                 Summary                                               Text  \n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
              "4            Great taffy  Great taffy at a great price.  There was a wid...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw = pd.read_csv('./data/Reviews.csv')\n",
        "print(df_raw.shape)\n",
        "df_raw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Transform: Clean data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect information about the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      568454 non-null  int64 \n",
            " 1   ProductId               568454 non-null  object\n",
            " 2   UserId                  568454 non-null  object\n",
            " 3   ProfileName             568428 non-null  object\n",
            " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
            " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
            " 6   Score                   568454 non-null  int64 \n",
            " 7   Time                    568454 non-null  int64 \n",
            " 8   Summary                 568427 non-null  object\n",
            " 9   Text                    568454 non-null  object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 43.4+ MB\n"
          ]
        }
      ],
      "source": [
        "# Check info\n",
        "df_raw.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
              "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check columns\n",
        "df_raw.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Id                         int64\n",
              "ProductId                 object\n",
              "UserId                    object\n",
              "ProfileName               object\n",
              "HelpfulnessNumerator       int64\n",
              "HelpfulnessDenominator     int64\n",
              "Score                      int64\n",
              "Time                       int64\n",
              "Summary                   object\n",
              "Text                      object\n",
              "dtype: object"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check data types\n",
        "df_raw.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The presence of missing values can be checked using .isnull().sum(), which counts the number of null entries in each column. If all columns return 0, it means there are no missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Id                         0\n",
              "ProductId                  0\n",
              "UserId                     0\n",
              "ProfileName               26\n",
              "HelpfulnessNumerator       0\n",
              "HelpfulnessDenominator     0\n",
              "Score                      0\n",
              "Time                       0\n",
              "Summary                   27\n",
              "Text                       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 0 missing values in critical columns such as  Text and Score. This makes it so there is no need to drop any entries. There are 26 missing ProfileName values which can be filled by 'Anonymous'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw['ProfileName'] = df_raw['ProfileName'].fillna('Anonymous')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 27 missing values in Summary. This isn't a critical column either so we can fill with empty string so its cleaner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw['Summary'] = df_raw['Summary'].fillna('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check again for missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Id                        0\n",
              "ProductId                 0\n",
              "UserId                    0\n",
              "ProfileName               0\n",
              "HelpfulnessNumerator      0\n",
              "HelpfulnessDenominator    0\n",
              "Score                     0\n",
              "Time                      0\n",
              "Summary                   0\n",
              "Text                      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_raw.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will check for duplicates. Duplicates inflates results and bias the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reviews: 568,454\n",
            "Duplicate rows (all columns): 0\n",
            "Duplicate reviews (same user, product, time, text): 1,205\n",
            "Duplicate text only: 174,875\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total reviews: {len(df_raw):,}\")\n",
        "print(f\"Duplicate rows (all columns): {df_raw.duplicated().sum():,}\")\n",
        "print(f\"Duplicate reviews (same user, product, time, text): {df_raw.duplicated(subset=['UserId', 'ProductId', 'Time', 'Text']).sum():,}\")\n",
        "print(f\"Duplicate text only: {df_raw.duplicated(subset=['Text']).sum():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 1,205 duplicate reviews mean that the same person reviewed the same product at the same time with identical text. This is likely accidental duplicate submissions. We can remove these as they are genuine duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 1205 duplicate reviews\n"
          ]
        }
      ],
      "source": [
        "df_raw = df_raw.drop_duplicates(subset=['UserId', 'ProductId', 'Time', 'Text'], keep='first')\n",
        "print(f\"Removed {1205} duplicate reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check agsin for duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reviews: 567,249\n",
            "Duplicate reviews (same user, product, time, text): 0\n",
            "Duplicate text only: 173,670\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total reviews: {len(df_raw):,}\")\n",
        "print(f\"Duplicate reviews (same user, product, time, text): {df_raw.duplicated(subset=['UserId', 'ProductId', 'Time', 'Text']).sum():,}\")\n",
        "print(f\"Duplicate text only: {df_raw.duplicated(subset=['Text']).sum():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reviews: 567,249\n",
            "Texts appearing multiple times: 58,004\n",
            "\n",
            "Top 30 most duplicated reviews:\n",
            "======================================================================\n",
            " 1.    103x (506 words): 'Diamond Almonds<br />Almonds are a good source of magnesium. One ounce contain a...'\n",
            " 2.     38x (864 words): 'This review will make me sound really stupid, but whatever. I don't really care ...'\n",
            " 3.     38x (108 words): 'Stash Chamomile Herbal Tea is tea bags with dried, crushed chamomile flowers.<br...'\n",
            " 4.     28x (242 words): 'I'm addicted to salty and tangy flavors, so when I opened my first bag of Sea Sa...'\n",
            " 5.     26x (142 words): 'I'm not a tea drinker, so I'll start with that right off the bat.  However, I wa...'\n",
            " 6.     26x (30 words): 'I'd continue to buy but I'm moving over to more home made finger foods and away ...'\n",
            " 7.     25x (131 words): 'I drank this tea at least 4 days a week during the last 6 weeks of my pregnancy....'\n",
            " 8.     25x (32 words): 'I bought this for my wife who is pregnant, and she drinks it everyday and likes ...'\n",
            " 9.     25x (64 words): 'i really have to say I love this chamomile tea. Especially when I have a cough a...'\n",
            "10.     25x (24 words): 'Better than many I have tried.  With a little honey this is a terrific cup of te...'\n",
            "11.     25x (59 words): 'I ordered this for my boyfriend because he has trouble sleeping sometimes. He lo...'\n",
            "12.     25x (52 words): 'I know nettle tea has a pretty distinct taste, but this particular brand was jus...'\n",
            "13.     25x (50 words): 'These reviews are wrong.It may work but I could not swallow this disgusting swil...'\n",
            "14.     25x (35 words): 'I dont do this as much as i should becuase it makes you poop all day!!!! but its...'\n",
            "15.     25x (20 words): 'the tea is very good with honey and I have seen a difference. I have lost 3lbs s...'\n",
            "16.     25x (56 words): 'I bought this to help me fall asleep and it works although it tastes and smell r...'\n",
            "17.     25x (21 words): 'Tastes Great!  My wife and I drink this tea on occasion and regular grocery stor...'\n",
            "18.     25x (29 words): 'This is a good tasting tea at a great price. If you don't normally like raspberr...'\n",
            "19.     25x (20 words): 'I really recommend this product. It is well packaged and protected. Also it smel...'\n",
            "20.     25x (42 words): 'I have used this product for quite some time on and off.  It is easy to use, dri...'\n",
            "21.     25x (65 words): 'This tea is weak at best when compared to other rose hip teas I have tried. The ...'\n",
            "22.     25x (85 words): 'This tea has a strong pungent smell that my husband hates (he says stinky socks)...'\n",
            "23.     25x (21 words): 'Excellent Tea. I enjoy a cup every now and then. It's really aromatic as well. S...'\n",
            "24.     25x (25 words): 'Worked great. A Good must have for breastfeeding mothers. Helped in restoring my...'\n",
            "25.     25x (34 words): 'The taste is not great, but it's bearable. I could not get used to the smell, th...'\n",
            "26.     25x (25 words): 'Love this herbal tea! It's so delicious! It tastes just like Oolong Tea from the...'\n",
            "27.     25x (29 words): 'I love this tea.. It was recomended to me by my Dr. to help during pregnancy.. D...'\n",
            "28.     25x (50 words): 'I have used this tea for 2 of my 3 pregnancies. It's a little hard to 1st get us...'\n",
            "29.     25x (21 words): 'I do like herbal teas in general and this fennel seed tea has a very pleasant ar...'\n",
            "30.     25x (50 words): 'I have a hard time finding nettle leaf tea at regular grocery stores, so I purch...'\n"
          ]
        }
      ],
      "source": [
        "# Quick investigation - see what those duplicates are\n",
        "text_counts = df_raw['Text'].value_counts()\n",
        "duplicated = text_counts[text_counts > 1]\n",
        "\n",
        "print(f\"Total reviews: {len(df_raw):,}\")\n",
        "print(f\"Texts appearing multiple times: {len(duplicated):,}\\n\")\n",
        "\n",
        "print(\"Top 30 most duplicated reviews:\")\n",
        "print(\"=\"*70)\n",
        "for i, (text, count) in enumerate(duplicated.head(30).items(), 1):\n",
        "    words = len(text.split())\n",
        "    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
        "    print(f\"{i:2d}. {count:>6,}x ({words:>2} words): '{preview}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generic short reviews (\"Great!\", \"Love it!\"). These are long detailed reviews appearing several times which means there are clear signs of fake/bot reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: 567,249 reviews\n",
            "After: 533,650 reviews\n",
            "Removed: 33,599 reviews\n"
          ]
        }
      ],
      "source": [
        "# Remove any review text appearing 10+ times\n",
        "\n",
        "print(f\"Before: {len(df_raw):,} reviews\")\n",
        "\n",
        "# Get text frequency\n",
        "text_counts = df_raw['Text'].value_counts()\n",
        "\n",
        "# Find suspicious texts (appearing 10+ times)\n",
        "suspicious = text_counts[text_counts >= 10].index\n",
        "\n",
        "# Remove all but first occurrence\n",
        "for text in suspicious:\n",
        "    duplicates = df_raw[df_raw['Text'] == text].index[1:]  # Get all except first\n",
        "    df_raw = df_raw.drop(duplicates)\n",
        "\n",
        "print(f\"After: {len(df_raw):,} reviews\")\n",
        "print(f\"Removed: {567249 - len(df_raw):,} reviews\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reviews: 533,650\n",
            "Max duplicates: 9\n",
            "Texts with 10+ occurrences: 0\n"
          ]
        }
      ],
      "source": [
        "# Final check for duplicates\n",
        "text_counts = df_raw['Text'].value_counts()\n",
        "print(f\"Reviews: {len(df_raw):,}\")\n",
        "print(f\"Max duplicates: {text_counts.max()}\")\n",
        "print(f\"Texts with 10+ occurrences: {(text_counts >= 10).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning HTML with simple method...\n",
            "✓ HTML cleaned\n"
          ]
        }
      ],
      "source": [
        "# SIMPLE HTML REMOVAL\n",
        "import re\n",
        "\n",
        "def simple_clean_html(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Replace common HTML entities\n",
        "    text = text.replace('&amp;', '&')\n",
        "    text = text.replace('&lt;', '<')\n",
        "    text = text.replace('&gt;', '>')\n",
        "    text = text.replace('&quot;', '\"')\n",
        "    text = text.replace('&#39;', \"'\")\n",
        "    text = text.replace('&nbsp;', ' ')\n",
        "    text = text.replace('<br>', ' ')\n",
        "    text = text.replace('<br/>', ' ')\n",
        "    return text\n",
        "\n",
        "print(\"Cleaning HTML with simple method...\")\n",
        "df_raw['Text'] = df_raw['Text'].apply(simple_clean_html)\n",
        "df_raw['Summary'] = df_raw['Summary'].apply(simple_clean_html)\n",
        "print(\"✓ HTML cleaned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HTML cleaning removes code/formatting tags that shouldn't be in review text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixing helpfulness ratios...\n",
            "Invalid ratios found: 2\n",
            "✓ Fixed 2 invalid ratios\n",
            "\n",
            "Final dataset: 533,650 reviews\n"
          ]
        }
      ],
      "source": [
        "# Fix invalid helpfulness ratios\n",
        "print(\"Fixing helpfulness ratios...\")\n",
        "\n",
        "# Find invalid ratios (numerator > denominator)\n",
        "invalid = df_raw['HelpfulnessNumerator'] > df_raw['HelpfulnessDenominator']\n",
        "\n",
        "print(f\"Invalid ratios found: {invalid.sum():,}\")\n",
        "\n",
        "if invalid.sum() > 0:\n",
        "    # Fix by capping numerator at denominator\n",
        "    df_raw.loc[invalid, 'HelpfulnessNumerator'] = df_raw.loc[invalid, 'HelpfulnessDenominator']\n",
        "    print(f\"✓ Fixed {invalid.sum():,} invalid ratios\")\n",
        "else:\n",
        "    print(\"✓ No invalid ratios - data is clean!\")\n",
        "\n",
        "print(f\"\\nFinal dataset: {len(df_raw):,} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this stage, after handling missing values, identifying and removing duplicates, and performing HTML content cleaning, we decided it was best to sample the dataset before proceeding with further analysis. This approach allows us to validate the effectiveness of our cleaning steps, ensure data consistency, and efficiently assess the overall data quality before applying more complex transformations or analytical processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating stratified sample of 20,000...\n",
            "✅ Sample created: 20,000 reviews\n"
          ]
        }
      ],
      "source": [
        "# Stratified sample\n",
        "print(\"Creating stratified sample of 20,000...\")\n",
        "\n",
        "df_sample = df_raw.groupby('Score', group_keys=False).apply(\n",
        "    lambda x: x.sample(min(len(x), int(20000 * len(x) / len(df_raw))), random_state=42)\n",
        ")\n",
        "\n",
        "# If slightly under 20k due to rounding, top up\n",
        "if len(df_sample) < 20000:\n",
        "    remaining = df_raw[~df_raw.index.isin(df_sample.index)]\n",
        "    extra = remaining.sample(n=20000 - len(df_sample), random_state=42)\n",
        "    df_sample = pd.concat([df_sample, extra])\n",
        "\n",
        "print(f\"✅ Sample created: {len(df_sample):,} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stratified sampling helps you get a more representative sample of the entire dataset, especially when certain subgroups are small or very different from others. This stratifies sampling is based on Score as it maintains the same proportion of 1-5 star ratings in the sample as in the full dataset.\n",
        "This was the chosen category as the sample:\n",
        "* Keeps real-world distribution\n",
        "* Not biased toward any rating\n",
        "* Analysis results will match full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20000, 10)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check sample shape\n",
        "df_sample.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Converted Time to Date\n",
            "\n",
            "Date range: 2000-01-03 00:00:00 to 2012-10-26 00:00:00\n",
            "\n",
            "First few dates:\n",
            "              Time       Date\n",
            "406805  1342396800 2012-07-16\n",
            "75917   1310947200 2011-07-18\n",
            "400894  1274486400 2010-05-22\n",
            "480704  1350086400 2012-10-13\n",
            "103599  1307404800 2011-06-07\n"
          ]
        }
      ],
      "source": [
        "# Convert Unix timestamp to datetime\n",
        "df_sample['Date'] = pd.to_datetime(df_sample['Time'], unit='s')\n",
        "\n",
        "print(\"✅ Converted Time to Date\")\n",
        "print(f\"\\nDate range: {df_sample['Date'].min()} to {df_sample['Date'].max()}\")\n",
        "print(f\"\\nFirst few dates:\")\n",
        "print(df_sample[['Time', 'Date']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above converts the Unix timestamps in the Time column into human-readable datetime values and stores them in a new column called Date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Sentiment column added!\n",
            "Sentiment\n",
            "Positive    15618\n",
            "Negative     2886\n",
            "Neutral      1496\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Add Sentiment column\n",
        "df_sample['Sentiment'] = df_sample['Score'].apply(lambda x: 'Negative' if x <= 2 else ('Neutral' if x == 3 else 'Positive'))\n",
        "\n",
        "print(\"✅ Sentiment column added!\")\n",
        "print(df_sample['Sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code above creates a new column called Sentiment based on the numerical values in the Score column.\n",
        "Each score is mapped to a sentiment category using a simple rule:\n",
        "\n",
        "* Scores 1 or 2 = “Negative”\n",
        "\n",
        "* Score 3 = “Neutral”\n",
        "\n",
        "* Scores 4 or 5 = “Positive”\n",
        "\n",
        "This transformation converts numerical ratings into qualitative sentiment labels, making the data easier to interpret and analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load: Save data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have a cleaned data to be saved as a new csv file. We will save the full cleaned data and the sample of the cleaned data as separate files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save full cleaned data\n",
        "df_raw.to_csv('./data/Reviews_Cleaned.csv', index=False)\n",
        "\n",
        "# Save sample cleaned data\n",
        "df_sample.to_csv('./data/Reviews_Sample_Cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Changed work directory\n",
        "* Imported required libraries\n",
        "* Extracted data\n",
        "* Collected insights regarding the data\n",
        "* Cleaned data(missing values, duplicate values, HTML cleaning)\n",
        "* Sampled the dataset for further analysis\n",
        "* Converted Unix timestamps to readable date values\n",
        "* Created a new sentiment column based on review scores\n",
        "* Exported the cleaned data and processed data to new CSV files"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
